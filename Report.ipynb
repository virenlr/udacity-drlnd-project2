{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='C://Users/viren/Desktop/p2_continuous-control/Reacher_Windows_x86_64/Reacher.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "# states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "# scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "# while True:\n",
    "#     actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#     actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "#     env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#     next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#     rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#     dones = env_info.local_done                        # see if episode finished\n",
    "#     scores += env_info.rewards                         # update the score (for each agent)\n",
    "#     states = next_states                               # roll over states to next time step\n",
    "#     if np.any(dones):                                  # exit loop if episode finished\n",
    "#         break\n",
    "# print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have loaded our Unity environment and tested a random agent to ensure that everything works as expected. Now begins the code that will train and use a DDPG agent to solve the problem.\n",
    "\n",
    "Let us start by importing all the remaining files we need to make this happen. Please note that the line **from ddpg_agent import Agent** uses a modified version of the DDPG code used in the Pendulum project present in Udacity's Github repository for this nanodegree. This file (and its dependencies *Actor* and *Critic* from the *model.py* file) are also imported at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from ddpg_agent import Agent\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now instantiate the agent. The observation space consists of 33 variables corresponding to the position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector is a number between -1 and 1.\n",
    "\n",
    "All this information is passed to our agent in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size, random_seed=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of the Learning Algorithm**\n",
    "\n",
    "**Actor-Critic Methods** are at the intersection of value-based methods such as DQN and policy-based methods such as REINFORCE.\n",
    "\n",
    "Suppose a Deep Reinforcement Learning agent uses a Deep Neural Network to approximate a value function. In that case, the agent is said to be value-based, and if it uses it to approximate a policy, it is said to be policy-based. If the agent learns a value-function well, deriving a good policy from it is straightforward.\n",
    "\n",
    "We learned about using baselines to reduce the variance of policy-based agents. We can use a value function as the baseline; if we train a Neural Network to approximate a value-function and use it as a baseline, we will get a better baseline, further reducing the policy-based method's variance. Actor-Critic methods use value-based techniques to further reduce the variance of policy-based methods.\n",
    "\n",
    "We need to consider the bias-variance tradeoff in Reinforcement Learning when an agent tries to estimate value-functions or policies from returns. A return is calculated using a single trajectory. However, the value-functions we are trying to estimate are computed using the expectation of returns.\n",
    "\n",
    "Ways to estimate expected returns:\n",
    "\n",
    "-  Monte Carlo estimate: We roll out an episode and calculate the discounted total reward from the reward sequence. Here, we just add all the rewards up, whether they are discounted or not. When we have a collection of episodes, some will have trajectories that go through the same states. Each of these episodes can yield a different Monte-Carlo estimate for the same value-function. To calculate the value-function, we average the estimates. More estimates produce a better value-function.\n",
    "\n",
    "- Temporal Difference estimate: Say we're estimating a state-value function. To estimate the current state's value, we use a single reward sample and an estimate of the total discounted return the agent will obtain from the next state onwards. Thus, we are estimating with an estimate. In the early stages, the following state estimates will be terrible, but it improves with time.\n",
    "\n",
    "Monte Carlo estimates will have high variance because estimates for a state can vary significantly across episodes but are unbiased. After all, we are not estimating using estimates, only with the real rewards obtained.\n",
    "\n",
    "Temporal Difference estimates are low variance because we only compound a single time-step of randomness instead of a full rollout. However, the bias will be high because we are estimating with estimates (bootstrapping).\n",
    "\n",
    "In REINFORCE, the return G was calculated as the total discounted return. This way of calculating G, which is simply a Monte Carlo estimate, has high variance. We then used a baseline to reduce the variance of the algorithm. However, this baseline was also calculated using the Monte Carlo approach. Assuming we use Deep Learning to learn this baseline (i.e., Function Approximation), still provides an advantage- the power of generalization. When we encounter a new state S', irrespective of whether we had visited it, the DNN will potentially come up with better estimates since it has been trained to generalize from similar data.\n",
    "\n",
    "Suppose we train the baseline with Temporal Difference estimates instead of Monte Carlo estimates. In that case, we can say we have a Critic. This will introduce bias but will cause a decrease in variance, improving convergence and speeding up learning. In Actor-Critic methods, we are trying to reduce the high variance commonly associated with policy-based agents.\n",
    "\n",
    "Actor-Critic agents are more stable than value-based agents and need fewer samples than policy-based agents.\n",
    "\n",
    "An Actor-Critic agent uses Function Approximation to learn a policy and a value function. We use two Neural Networks here- one for the Actor and one for the Critic. The Critic will learn to evaluate the state-value function Vπ using the Temporal Difference estimate. Using the Critic, we calculate the Advantage Function and train the Actor using this value.\n",
    "\n",
    "In the case of a simple Actor-Critic network, the Actor takes in a state and outputs the distribution over actions. The Critic takes in a state and outputs a state-value function of policy π, Vπ. We start by inputting the current state into the Actor and get the action to take in that state. We observe the next state and reward to get the experience tuple (s, a, r, s'). Then, we train the Critic with the Temporal Difference estimate, which is the reward r plus the Critic's estimate for s'.\n",
    "\n",
    "(s, a, r, s') → r  + γV(s'; θv)\n",
    "\n",
    "To calculate the Advantage:\n",
    "\n",
    "Aπ(s, a) = r + γV(s'; θv) - V(s; θv) (Where V(s; θv) comes from the Critic).\n",
    "\n",
    "We then train the Actor using the calculated Advantage as the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DDPG (Deep Deterministic Policy Gradient):** This is a type of Actor-Critic method and can be seen as an approximate Deep Q-Network instead of an actual Actor-Critic. Here, the Critic is used to approximate the maximizer over the next state's Q-values and not as a learned baseline.\n",
    "\n",
    "One of the limitations of the Deep Q-Learning agent is that it is not straightforward to use in continuous action spaces. In DDPG, we use two neural networks- the \"Actor\" and the \"Critic\" (different from the terminology used before). The Actor is used to approximate the optimal policy deterministically- outputting the best-believed action for any given state. This is unlike stochastic policies, where we want the policy to learn a probability distribution over the actions. The Actor essentially learns the arg_max a Q(s, a), which is the best action. The Critic learns to evaluate the optimal action-value function by using the action the Actor believes to be the best. We use the Actor, an approximate maximizer, to calculate a new target value for training the action-value function in the way Deep Q-Networks do.\n",
    "\n",
    "DDPGs use Replay Buffers and make \"soft-updates\" to the target networks. In Deep Q-Networks, we maintain two copies of the network weights, the regular and target networks. The target network is updated, say, every 10,000-time steps, by copying the regular network's weights. In DDPGs, there are 4 networks- a regular and a target for the Actor and the Critic. The target networks are updated with soft-updates, that is, slowly blending the regular network weights with the target's, over a period of time, and not all at once. In practice, this yields faster convergence and can be used with other algorithms that use target networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of Hyperparameters used**\n",
    "\n",
    "`BUFFER_SIZE = int(1e5)`\n",
    "\n",
    "The size of the Replay Buffer that holds tuples of experiences that can be selected multiple times.\n",
    "\n",
    "`BATCH_SIZE = 128`\n",
    "\n",
    "The number of entries from the Replay Buffer that are considered a part of each batch.\n",
    "\n",
    "`GAMMA = 0.99`\n",
    "\n",
    "The hyperparameter that prioritizes how much weightage is given to recently received rewards, as compared to previous rewards.\n",
    "\n",
    "`TAU = 1e-3`\n",
    "\n",
    "The architecture makes use of two networks (for the Actor and Critic, each)- a fixed network and a target network. This hyperparameter is used for providing soft updates to the target network; i.e., instead of updating the values all at once, the process happens gradually, controlled with this hyperparameter.\n",
    "\n",
    "`LR_ACTOR = 1e-4`\n",
    "\n",
    "The learning rate with which the Actor model learns.\n",
    "\n",
    "`LR_CRITIC = 1e-3`\n",
    "\n",
    "The learning rate with which the Critic model learns.\n",
    "\n",
    "`WEIGHT_DECAY = 0`\n",
    "\n",
    "The L2 weight decay used to enhance the speed of convergence.\n",
    "\n",
    "`NUM_LEARN_UPDATES = 10`\n",
    "\n",
    "The number of epochs of learning that happen at update time.\n",
    "\n",
    "`NUM_TIME_STEPS_TO_UPDATE = 20`\n",
    "\n",
    "After how many time steps to update the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of Model Architecture**\n",
    "\n",
    "There are a total of four networks- a regular and target for the Actor, and a regular and target for the Critic. All networks have identical architectures as follows:\n",
    "\n",
    "They consist of an input layer with 33 nodes (for the 33 dimension vector provided as the state), followed by two layers of 400 and 300 neurons respectively. ReLu activations are used to maintain the non-linearity in the models. The output layer consists of four nodes, representing a vector of the continuous action the agent must take. Tanh activations at this layer ensure that the output lies between -1 and +1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of Training Process**\n",
    "\n",
    "Here, I will outline the steps I took before arriving at a satisfactory model architecture.\n",
    "\n",
    "Trial 1: I started with the single-agent environment and directly ported the DDPG code from Udacity's GitHub repository corresponding to the Pendulum project. While the program ran with little modification, the agent refused to learn.\n",
    "\n",
    "I tried everything- adding Batch Normalization after the first layer, tweaks in the hyperparameters, and running the training for 2000 episodes. Nothing worked. I gave up and switched over to the multi-agent environment.\n",
    "\n",
    "Trial 2: I had to make some changes to the ddpg_agent.py file to adapt the code to work with 20 agents in parallel.\n",
    "\n",
    "I added two new hyperparameters `NUM_LEARN_UPDATES = 10` and `NUM_TIME_STEPS_TO_UPDATE = 20` with the hints provided in the classroom.\n",
    "\n",
    "I also had to update the `step` method to 1. save the experiences of all 20 agents in the buffer, 2. update the models on every 20th time step, and 3. perform 10 epochs of updation during update-time.\n",
    "\n",
    "```\n",
    "# Save experience / reward for ALL the 20 agents:\n",
    "\n",
    "for i in range (20):\n",
    "    self.memory.add(state[i,:], action[i,:], reward[i], next_state[i,:], done[i])\n",
    "    \n",
    "# The following lines of code ensure that updates only take place on every Nth specified time step\n",
    "\n",
    "if time_step % NUM_TIME_STEPS_TO_UPDATE != 0:\n",
    "    return\n",
    "    \n",
    "# The loop here causes NUM_LEARN_UPDATES epochs of learning to happen at updation time\n",
    "\n",
    "for i in range(NUM_LEARN_UPDATES):\n",
    "    experiences = self.memory.sample()\n",
    "    self.learn(experiences, GAMMA)\n",
    "```\n",
    "\n",
    "Lastly, based on a suggestion I saw in a [GitHub pull request](https://github.com/udacity/deep-reinforcement-learning/pull/19) to the project, I changed the `sample` method in the OUNoise class.\n",
    "\n",
    "```\n",
    "dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal((20, 4)) # 20 agents and 4 actions\n",
    "```\n",
    "\n",
    "With these changes, and with removing the max number of time steps per episode from the below method, the agent trained perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 10.15\n",
      "Episode 182\tAverage Score: 30.08\n",
      "Environment solved in 82 episodes!\tAverage Score: 30.08\n"
     ]
    }
   ],
   "source": [
    "def ddpg(n_episodes=1000, print_every=100):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    avg_score_tracker = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        # Instead of saying: state = env.reset(), we must first create an env_info object as shown above\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        \n",
    "        agent.reset()\n",
    "        scores = np.zeros(num_agents)                                  # initialize the score (for each agent)\n",
    "        \n",
    "        t = 0\n",
    "        \n",
    "        while(True):\n",
    "            t = t+1\n",
    "            actions = agent.act(states)                                # select an action (for each agent)\n",
    "            \n",
    "            # Instead of saying: next_state, reward, done, _ = env.step(action), we must do this step as shown above\n",
    "            \n",
    "            env_info = env.step(actions)[brain_name]                   # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations                 # get next states (for each agent)\n",
    "            rewards = env_info.rewards                                 # get the reward (for each agent)\n",
    "            dones = env_info.local_done                                # see if episode has finished\n",
    "            \n",
    "            # Pass in the time step as well so the agent knows when it needs to update\n",
    "            agent.step(t, states, actions, rewards, next_states, dones)\n",
    "            \n",
    "            states = next_states                                       # roll over states to next time step\n",
    "            scores += rewards                                          # update the score (for each agent)\n",
    "            \n",
    "            if np.any(dones):                                          # exit loop if episode finished\n",
    "                break\n",
    "                \n",
    "        scores_deque.append(np.mean(scores))\n",
    "        avg_score_tracker.append(np.mean(scores))\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)), end=\"\")\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            \n",
    "        if np.mean(scores_deque)>=30.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            break\n",
    "            \n",
    "    return avg_score_tracker\n",
    "\n",
    "scores = ddpg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Plot of Rewards\n",
    "\n",
    "This plot of rewards per episode has been included to illustrate that the agents, on average, are capable of receiving an average reward of +30 over the last 100 episodes. From the previous code cell, we see that we have solved the environment in **82** episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now go ahead and close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Ideas for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having gone through Udacity's *Knowledge* portal, I see there are many things I can experiment with:\n",
    "\n",
    "- Some people have gotten better performance with using a bigger buffer or minibatch size, different learning rates, and different architectures for the DNNs that correspond to the Actor and Critic.\n",
    "\n",
    "- Other algorithms covered in the lecture like A3C, A2C, and GAE.\n",
    "\n",
    "- Adapting the code to work with the Unity Crawler environment that involves not one, but four arms, working in tandem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
